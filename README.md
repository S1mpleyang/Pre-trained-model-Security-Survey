# Pre-trained-model-Security-Survey

A collection of papers and resources related to security issuses of Pre-trained Models.

The organization of papers refers to our survey ["New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook"](https://arxiv.org/abs/2411.07691).

Please let us know if you find out a mistake or have any suggestions by email: meng.yang-4@student.uts.edu.au

If you find our survey useful for your research, please cite the following paper:

```
@article{yang2024new,
  title={New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook},
  author={Yang, Meng and Zhu, Tianqing and Liu, Chi and Zhou, WanLei and Yu, Shui and Yu, Philip S},
  journal={arXiv preprint arXiv:2411.07691},
  year={2024}
}
```


# Table of Contents

- [LLMSurvey](#Pre-trained-model-Security-Survey)
  - [Table of Contents](#table-of-contents)
  - [Related Sources](#related-sources)
    - [Attack](#attack)
      - [No-Change Attacks](#No-Change-Attacks)
      - [Input-Change Attacks](#Input-Change-Attacks)
      - [Model-Change Attacks](#Model-Change-Attacks)
    - [Defense](#defense)
      - [No-Change Defenses](#No-Change-Defenses)
      - [Input-Change Defenses](#Input-Change-Defenses)
      - [Model-Change Defenses](#Model-Change-Defenses)

# Related Sources

## Attack
### No-Change Attacks

### Input-Change Attacks

### Model-Change Attacks

1. <u>GShard</u>: **"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"**. *Dmitry Lepikhin et al.* ICLR 2021. [[Paper](http://arxiv.org/abs/2006.16668v1)]


## Defense
### No-Change Defenses

### Input-Change Defenses

### Model-Change Defenses


