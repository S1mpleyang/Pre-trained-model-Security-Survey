# Pre-trained-model-Security-Survey

A collection of papers and resources related to security issuses of Pre-trained Models.

The organization of papers refers to our survey ["New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook"](https://arxiv.org/abs/2411.07691).

Please let us know if you find out a mistake or have any suggestions by email: meng.yang-4@student.uts.edu.au

If you find our survey useful for your research, please cite the following paper:

```
@article{yang2024new,
  title={New Emerged Security and Privacy of Pre-trained Model: a Survey and Outlook},
  author={Yang, Meng and Zhu, Tianqing and Liu, Chi and Zhou, WanLei and Yu, Shui and Yu, Philip S},
  journal={arXiv preprint arXiv:2411.07691},
  year={2024}
}
```


## Table of Contents

- [LLMSurvey](#Pre-trained-model-Security-Survey)
  - [Table of Contents](#table-of-contents)
  - [List of Pre-trained Models as Attack Target](List-of-Pre-trained-Models-as-Attack-Target)
  - [Related Sources](#related-sources)
    - [Attack](#attack)
      - [No-Change Attacks](#No-Change-Attacks)
      - [Input-Change Attacks](#Input-Change-Attacks)
      - [Model-Change Attacks](#Model-Change-Attacks)
    - [Defense](#defense)
      - [No-Change Defenses](#No-Change-Defenses)
      - [Input-Change Defenses](#Input-Change-Defenses)
      - [Model-Change Defenses](#Model-Change-Defenses)

## List of Pre-trained Models as Attack Target

<table class="tg">
<thead>
  <tr>
    <th class="tg-nrix" align="center" rowspan="2">Categories</th>
    <th class="tg-0lax" align="center" rowspan="2">Model</th>
    <th class="tg-baqh" align="center" rowspan="2">Available</th>
    <th class="tg-0lax" align="center" rowspan="2">Modal</th>
    <th class="tg-baqh" align="center" rowspan="2">Size(B)</th>
    <th class="tg-0lax" align="center" rowspan="2">Base Model</th>
    <th class="tg-baqh" align="center" rowspan="2">Release Time</th>
  </tr>
  <tr>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td class="tg-nrix" align="center" rowspan="27"> Small <br> Pre-trained Model</td>
    <td class="tg-0lax" align="center">GPT-1</td>
    <td class="tg-baqh" align="center">Open-source</td>
    <td class="tg-0lax" align="center">text</td>
    <td class="tg-baqh" align="center">-</td>
    <td class="tg-0lax" align="center">Transformer (decoder)</td>
    <td class="tg-baqh" align="center">Jun-2018</td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">BERT</td>
    <td class="tg-baqh" align="center">Open-source</td>
    <td class="tg-0lax" align="center">text</td>
    <td class="tg-baqh" align="center">330MB</td>
    <td class="tg-0lax" align="center">Transformer (Encoder)</td>
    <td class="tg-baqh" align="center">Oct-2018</td>
  </tr>
 <tr>
    <td class="tg-0lax" align="center">GPT-3</td>
    <td class="tg-baqh" align="center">Close-source</td>
    <td class="tg-0lax" align="center">text</td>
    <td class="tg-baqh" align="center">6B/175B</td>
    <td class="tg-0lax" align="center">GPT-2</td>
    <td class="tg-baqh" align="center">May-2020</td>
  </tr>
  <tr>
    <td class="tg-0lax" align="center">GPT-3.5</td>
    <td class="tg-baqh" align="center">Close-source</td>
    <td class="tg-0lax" align="center">text</td>
    <td class="tg-baqh" align="center">-</td>
    <td class="tg-0lax" align="center">GPT-3</td>
    <td class="tg-baqh" align="center">Mar-2022</td>
  </tr>
</tbody>
</table>

  

## Related Sources

### Attack
#### No-Change Attacks

#### Input-Change Attacks

#### Model-Change Attacks

1. <u>GShard</u>: **"GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding"**. *Dmitry Lepikhin et al.* ICLR 2021. [[Paper](http://arxiv.org/abs/2006.16668v1)]


### Defense
#### No-Change Defenses

#### Input-Change Defenses

#### Model-Change Defenses


